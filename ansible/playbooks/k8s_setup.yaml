---
- hosts: all
  become: true
  vars:
    packages:
      - gnupg2
      - apt-transport-https
      - software-properties-common
      - curl
      - ca-certificates
  tasks:
    - name: Change hostname to private DNS name
      ansible.builtin.shell: hostnamectl set-hostname "{{ hostvars[inventory_hostname]['private_dns_name'] }}"
      become: true  
      
    - name: Remove Keyrings Directory (if it exists)
      ansible.builtin.shell: rm -rf /etc/apt/keyrings

    - name: Remove Existing Kubernetes Directory (if it exists)
      ansible.builtin.shell: sudo rm -rf /etc/apt/sources.list.d/pkgs_k8s_io_core_stable_v1_30_deb.list

    - name: Disable swap
      ansible.builtin.command:
        cmd: swapoff -a

    - name: Ensure swap is disabled on boot
      ansible.builtin.command:
        cmd: sudo sed -i -e '/\/swap.img\s\+none\s\+swap\s\+sw\s\+0\s\+0/s/^/#/' /etc/fstab

    - name: Add kernel modules for Containerd
      ansible.builtin.copy:
        dest: /etc/modules-load.d/containerd.conf
        content: |
          overlay
          br_netfilter

    - name: Load kernel modules for Containerd
      ansible.builtin.shell:
        cmd: modprobe overlay && modprobe br_netfilter
      become: true
    
    

    - name: Enable bridged traffic for Kubernetes networking
      ansible.builtin.copy:
        dest: /etc/sysctl.d/k8s.conf
        content: |
          net.bridge.bridge-nf-call-ip6tables = 1
          net.bridge.bridge-nf-call-iptables = 1
        mode: '0644'

    - name: Reload sysctl settings
      ansible.builtin.command: sysctl --system

    - name: Update apt cache
      ansible.builtin.apt:
        update_cache: yes
        cache_valid_time: 3600

    - name: Install required packages for apt over HTTPS and curl
      ansible.builtin.apt:
        name: "{{ packages }}"
        state: present

    # Updated Kubernetes installation method with official repo and keyrings
    - name: Create /etc/apt/keyrings directory
      ansible.builtin.file:
        path: /etc/apt/keyrings
        state: directory
        mode: '0755'

    - name: Download Kubernetes Release key and dearmor it
      ansible.builtin.shell: >
        curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg
      args:
        creates: /etc/apt/keyrings/kubernetes-apt-keyring.gpg    

    - name: Set permissions on the Kubernetes apt keyring
      ansible.builtin.file:
        path: /etc/apt/keyrings/kubernetes-apt-keyring.gpg
        mode: '0644'

    - name: Add Kubernetes apt repository
      ansible.builtin.apt_repository:
        repo: "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /"
        state: present
        filename: kubernetes

    - name: Set permissions on Kubernetes sources list
      ansible.builtin.file:
        path: /etc/apt/sources.list.d/kubernetes.list
        mode: '0644'    

    - name: Update apt cache after adding Kubernetes repo
      ansible.builtin.apt:
        update_cache: yes

    - name: Install kubelet, kubeadm, kubectl and docker.io
      ansible.builtin.apt:
        name:
          - kubelet
          - kubeadm
          - kubectl
          - docker.io
        state: present

    - name: Hold kubelet, kubeadm, kubectl packages
      ansible.builtin.command:
        cmd: sudo apt-mark hold kubelet kubeadm kubectl
        

    - name: Add user 'ubuntu' to docker group
      ansible.builtin.user:
        name: ubuntu
        groups: docker
        append: yes

    - name: Restart and enable Docker service
      ansible.builtin.service:
        name: docker
        state: restarted
        enabled: yes

    - name: Configure Docker daemon to use systemd as cgroup driver
      ansible.builtin.copy:
        dest: /etc/docker/daemon.json
        content: |
          {
            "exec-opts": ["native.cgroupdriver=systemd"]
          }
        mode: '0644'
      notify:
        - Restart Docker
        - Restart Kubelet

  handlers:
    - name: Restart Docker
      ansible.builtin.systemd:
        name: docker
        state: restarted
        daemon_reload: yes

    - name: Restart Kubelet
      ansible.builtin.systemd:
        name: kubelet
        state: restarted

- hosts: masters
  become: true
  tasks:
    - name: Pull Kubernetes images before installation
      ansible.builtin.command: kubeadm config images pull

    - name: Copy cluster configuration base file
      ansible.builtin.copy:
        src: /var/lib/jenkins/workspace/test-playbook-to-create-k8s-cluster/ansible/playbooks/clusterconfig-base.yaml
        dest: /home/ubuntu/clusterconfig-base.yml
        owner: ubuntu
        group: ubuntu
        mode: '0644'

    - name: Install gettext-base package
      ansible.builtin.apt:
        name: gettext-base
        state: present

    - name: Produce clusterconfig.yml with control plane endpoint substituted
      ansible.builtin.shell: |
        export CONTROLPLANE_ENDPOINT={{ hostvars[inventory_hostname]['private_ip_address'] }}
        envsubst < /home/ubuntu/clusterconfig-base.yml > /home/ubuntu/clusterconfig.yml
      args:
        executable: /bin/bash
      changed_when: false  # to avoid rerunning unnecessarily
    - name: Initialize Kubernetes cluster using kubeadm
      ansible.builtin.command: kubeadm init --config /var/lib/jenkins/petclinic-microservices-with-db/test-playbook-to-create-k8s-cluster/ansible/playbooks/clusterconfig-base.yaml
      args:
        creates: /etc/kubernetes/admin.conf

    - name: Setup kubeconfig for ubuntu user
      ansible.builtin.file:
        path: /home/ubuntu/.kube
        state: directory
        owner: ubuntu
        group: ubuntu
        mode: '0700'

    - name: Copy admin.conf to user's kubeconfig location
      ansible.builtin.copy:
        src: /etc/kubernetes/admin.conf
        dest: /home/{{ ansible_user }}/.kube/config
        owner: "{{ ansible_user }}"
        group: "{{ ansible_user }}"
        mode: '0644'
        remote_src: yes

    - name: Install Flannel pod network
      ansible.builtin.command: kubectl apply -f https://github.com/coreos/flannel/raw/master/Documentation/kube-flannel.yml
      environment:
        KUBECONFIG: /etc/kubernetes/admin.conf
      changed_when: false

    - name: Get join command from master
      shell: kubeadm token create --print-join-command
      register: join_command_raw
      run_once: true

    - name: Set join command fact for all hosts
      set_fact:
        kubeadm_join_command: "{{ join_command_raw.stdout }}"
      run_once: true
      delegate_to: localhost

- hosts: workers
  become: true
  tasks:
    - name: Delete node from cluster if exists
      delegate_to: 54.174.135.230
      command: kubectl delete node {{ inventory_hostname }}
      ignore_errors: yes

    - name: Reset Kubernetes on worker node
      become: yes
      command: kubeadm reset -f

    - name: Remove Kubernetes CNI configs
      become: yes
      file:
        path: /etc/cni/net.d
        state: absent

    - name: Stop kubelet
      become: yes
      service:
        name: kubelet
        state: stopped
        enabled: no

    - name: Restart container runtime
      become: yes
      service:
        name: containerd
        state: restarted

    - name: Join worker nodes to cluster
      ansible.builtin.shell: "{{ hostvars[groups['role_master'][0]].kubeadm_join_command }}"
      register: joining_result
      changed_when: joining_result.rc == 0
      failed_when: joining_result.rc != 0 and 'already a member of the cluster' not in joining_result.stderr

- hosts: masters
  become: false
  tasks:
    - name: Patch master and worker nodes with AWS providerID
      ansible.builtin.shell: |
        kubectl patch node {{ hostvars[groups['role_master'][0]]['private_dns_name'] }} -p '{"spec":{"providerID":"aws:///us-east-1a/{{ hostvars[groups['role_master'][0]]['instance_id'] }}" }}'
        kubectl patch node {{ hostvars[groups['role_worker'][0]]['private_dns_name'] }} -p '{"spec":{"providerID":"aws:///us-east-1a/{{ hostvars[groups['role_worker'][0]]['instance_id'] }}" }}'
        kubectl patch node {{ hostvars[groups['role_worker'][1]]['private_dns_name'] }} -p '{"spec":{"providerID":"aws:///us-east-1a/{{ hostvars[groups['role_worker'][1]]['instance_id'] }}" }}'
      args:
        chdir: /home/ubuntu

    - name: Download Helm install script
      ansible.builtin.get_url:
        url: https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
        dest: /home/ubuntu/get_helm.sh
        mode: '0755'

    - name: Run Helm install script
      ansible.builtin.command: /home/ubuntu/get_helm.sh
      args:
        creates: /usr/local/bin/helm

    - name: Add and update AWS cloud-controller-manager Helm repo
      ansible.builtin.command: helm repo add aws-cloud-controller-manager https://kubernetes.github.io/cloud-provider-aws
      register: helm_repo_added
      failed_when: false

    - name: Update Helm repositories
      ansible.builtin.command: helm repo update
      when: helm_repo_added is succeeded

    - name: Install or upgrade AWS cloud-controller-manager via Helm
      ansible.builtin.command: >
        helm upgrade --install aws-cloud-controller-manager aws-cloud-controller-manager/aws-cloud-controller-manager
        --set image.tag=v1.20.0-alpha.0

    - name: Deploy NGINX ingress controller for AWS
      ansible.builtin.command: kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.1.2/deploy/static/provider/aws/deploy.yaml

    - name: Deploy AWS EBS CSI Driver
      ansible.builtin.command: >
        kubectl apply -k github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable?ref=release-0.13

    - name: Copy storage.yml to remote
      ansible.builtin.copy:
        src: ./storage.yml
        dest: /home/ubuntu/storage.yml
        owner: ubuntu
        group: ubuntu
        mode: '0644'

    - name: Apply StorageClass configuration
      ansible.builtin.command: kubectl apply -f /home/ubuntu/storage.yml

     #######################################
     # Cluster Cleanup / Tear-down Section #
     #######################################

    - name: Reset Kubernetes on worker nodes
      hosts: workers
      become: true
      tasks:
        - name: Reset kubeadm
          command: kubeadm reset -f

        - name: Remove Kubernetes CNI configs
          file:
            path: /etc/cni/net.d
            state: absent

        - name: Stop kubelet
          service:
            name: kubelet
            state: stopped
            enabled: no

        - name: Restart container runtime
          service:
            name: docker
            state: restarted
            enabled: yes

        - name: Reset Kubernetes on master nodes
          hosts: masters
          become: true
          tasks:
        - name: Reset kubeadm
          command: kubeadm reset -f

        - name: Remove Kubernetes directories
          file:
            path: "{{ item }}"
            state: absent
          loop:
            - /etc/kubernetes
            - /var/lib/etcd

        - name: Stop kubelet
          service:
            name: kubelet
            state: stopped
            enabled: no

        - name: Restart container runtime
          service:
            name: docker
            state: restarted
            enabled: yes 